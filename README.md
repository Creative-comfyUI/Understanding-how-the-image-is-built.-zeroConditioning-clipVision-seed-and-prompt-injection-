<h1>Understanding how the image is built. zeroConditioning, clipVision, seed and prompt injection </h1>
Are we sure we understand how the image is built and what reference the prompt image is based on?

Since [@cubiq](https://github.com/cubiq/prompt_injection) creation of the prompt injection node, I have discovered that what I thought about image creation in comfyUI is probably not what I imagined.  

Before I begin my demonstration. I invite you to read the first part of Prompt Injection (https://github.com/Creative-comfyUI/prompt_injection) . You can also read what I have written about seeds (https://github.com/Creative-comfyUI/seed_eng). All this information is necessary to understand what I am about to demonstrate. 

In my previous repisotery about prompt injection, <b>I explained that even prompt is muted with zeroConditioning, ksampler generated image </b>. <b>This image looks like a reference</b>. You have to understand that no text in the clip text encoding doesn't mean that the prompt is muted. The zeroConditioning ensures that no text information is used by the clip (https://openai.com/index/clip/). 

The image generated by zeroConditioning is a reference image. There is something tricky about. Using batch size 3 show us that there is more than one reference image. One is a reference at the beginning of the process and the other one is a reference for the end of the process. You think I am crazy? Not at all. Let us see with examples.

Before, it is important to understand for some reasons that <b>Sdxl Turbo, Lightning and in general few steps model give the best result and more accurate in all the process</b>. For all my examples I will use the sdxl model. SD 1.5 may be different. <b>I will use Euler as I explain before Euler always give the same image for a batch size</b>. I'm also using Ksampler Advanced (inspire) to make sure that comfyUI doesn't add noise variation.

First I will generate an image with a few steps model with a radom seed. Size 832 * 1216 . Batch size 1 

Here is the result  
![Screenshot 2024-06-13 at 1 55 00 PM](https://github.com/Creative-comfyUI/Understanding-how-the-image-is-built.-zeroConditioning-clipVision-seed-and-prompt-injection-/assets/166729777/bf267d74-a5a0-4fa4-be49-8c068cec0738)

We will keep the same seed and fix it and generate a batch size of 3 this time. 

Here is the result 

![Screenshot 2024-06-13 at 2 04 18 PM](https://github.com/Creative-comfyUI/Understanding-how-the-image-is-built.-zeroConditioning-clipVision-seed-and-prompt-injection-/assets/166729777/1e0cc15b-5a81-401e-b871-f4d638db8a4f)

Everything is fine as we have the same images as we use Euler

Now we are going to use prompt injection to see what has happened.

We will link an empty clip Text Encode to output 8.  

Here is the result 

![Screenshot 2024-06-13 at 3 19 55 PM](https://github.com/Creative-comfyUI/Understanding-how-the-image-is-built.-zeroConditioning-clipVision-seed-and-prompt-injection-/assets/166729777/dea90ee1-5324-4d49-99ef-d10785cd06a7)

<b>The third image is different from the others. This is what I called the second reference</b>. 

Now we are going to write a text in the clip prompt injection of output 8.

